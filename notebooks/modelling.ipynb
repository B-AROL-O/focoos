{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fai-detr-l-obj365', 'fai-detr-l-coco', 'fai-detr-m-coco', 'fai-detr-s-coco', 'fai-detr-n-coco', 'fai-mf-l-ade', 'fai-mf-m-ade', 'fai-mf-l-coco-ins', 'fai-mf-m-coco-ins', 'fai-mf-s-coco-ins', 'bisenetformer-m-ade']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[05/22 07:51][INFO][model_info]: \n",
      "            üìã Name: fai-detr-l-obj365\n",
      "            üìù Description: RTDETR Large model (Object365)\n",
      "            üë™ Family: ModelFamily.DETR\n",
      "            üîó Focoos Model: fai-detr-l-obj365\n",
      "            üéØ Task: Task.DETECTION\n",
      "            üè∑Ô∏è Classes: ['Person', 'Sneakers', 'Chair', 'Other Shoes', 'Hat', 'Car', 'Lamp', 'Glasses', 'Bottle', 'Desk', 'Cup', 'Street Lights', 'Cabinet/shelf', 'Handbag/Satchel', 'Bracelet', 'Plate', 'Picture/Frame', 'Helmet', 'Book', 'Gloves', 'Storage box', 'Boat', 'Leather Shoes', 'Flower', 'Bench', 'Potted Plant', 'Bowl/Basin', 'Flag', 'Pillow', 'Boots', 'Vase', 'Microphone', 'Necklace', 'Ring', 'SUV', 'Wine Glass', 'Belt', 'Moniter/TV', 'Backpack', 'Umbrella', 'Traffic Light', 'Speaker', 'Watch', 'Tie', 'Trash bin Can', 'Slippers', 'Bicycle', 'Stool', 'Barrel/bucket', 'Van', 'Couch', 'Sandals', 'Bakset', 'Drum', 'Pen/Pencil', 'Bus', 'Wild Bird', 'High Heels', 'Motorcycle', 'Guitar', 'Carpet', 'Cell Phone', 'Bread', 'Camera', 'Canned', 'Truck', 'Traffic cone', 'Cymbal', 'Lifesaver', 'Towel', 'Stuffed Toy', 'Candle', 'Sailboat', 'Laptop', 'Awning', 'Bed', 'Faucet', 'Tent', 'Horse', 'Mirror', 'Power outlet', 'Sink', 'Apple', 'Air Conditioner', 'Knife', 'Hockey Stick', 'Paddle', 'Pickup Truck', 'Fork', 'Traffic Sign', 'Ballon', 'Tripod', 'Dog', 'Spoon', 'Clock', 'Pot', 'Cow', 'Cake', 'Dinning Table', 'Sheep', 'Hanger', 'Blackboard/Whiteboard', 'Napkin', 'Other Fish', 'Orange/Tangerine', 'Toiletry', 'Keyboard', 'Tomato', 'Lantern', 'Machinery Vehicle', 'Fan', 'Green Vegetables', 'Banana', 'Baseball Glove', 'Airplane', 'Mouse', 'Train', 'Pumpkin', 'Soccer', 'Skiboard', 'Luggage', 'Nightstand', 'Tea pot', 'Telephone', 'Trolley', 'Head Phone', 'Sports Car', 'Stop Sign', 'Dessert', 'Scooter', 'Stroller', 'Crane', 'Remote', 'Refrigerator', 'Oven', 'Lemon', 'Duck', 'Baseball Bat', 'Surveillance Camera', 'Cat', 'Jug', 'Broccoli', 'Piano', 'Pizza', 'Elephant', 'Skateboard', 'Surfboard', 'Gun', 'Skating and Skiing shoes', 'Gas stove', 'Donut', 'Bow Tie', 'Carrot', 'Toilet', 'Kite', 'Strawberry', 'Other Balls', 'Shovel', 'Pepper', 'Computer Box', 'Toilet Paper', 'Cleaning Products', 'Chopsticks', 'Microwave', 'Pigeon', 'Baseball', 'Cutting/chopping Board', 'Coffee Table', 'Side Table', 'Scissors', 'Marker', 'Pie', 'Ladder', 'Snowboard', 'Cookies', 'Radiator', 'Fire Hydrant', 'Basketball', 'Zebra', 'Grape', 'Giraffe', 'Potato', 'Sausage', 'Tricycle', 'Violin', 'Egg', 'Fire Extinguisher', 'Candy', 'Fire Truck', 'Billards', 'Converter', 'Bathtub', 'Wheelchair', 'Golf Club', 'Briefcase', 'Cucumber', 'Cigar/Cigarette ', 'Paint Brush', 'Pear', 'Heavy Truck', 'Hamburger', 'Extractor', 'Extention Cord', 'Tong', 'Tennis Racket', 'Folder', 'American Football', 'earphone', 'Mask', 'Kettle', 'Tennis', 'Ship', 'Swing', 'Coffee Machine', 'Slide', 'Carriage', 'Onion', 'Green beans', 'Projector', 'Frisbee', 'Washing Machine/Drying Machine', 'Chicken', 'Printer', 'Watermelon', 'Saxophone', 'Tissue', 'Toothbrush', 'Ice cream', 'Hotair ballon', 'Cello', 'French Fries', 'Scale', 'Trophy', 'Cabbage', 'Hot dog', 'Blender', 'Peach', 'Rice', 'Wallet/Purse', 'Volleyball', 'Deer', 'Goose', 'Tape', 'Tablet', 'Cosmetics', 'Trumpet', 'Pineapple', 'Golf Ball', 'Ambulance', 'Parking meter', 'Mango', 'Key', 'Hurdle', 'Fishing Rod', 'Medal', 'Flute', 'Brush', 'Penguin', 'Megaphone', 'Corn', 'Lettuce', 'Garlic', 'Swan', 'Helicopter', 'Green Onion', 'Sandwich', 'Nuts', 'Speed Limit Sign', 'Induction Cooker', 'Broom', 'Trombone', 'Plum', 'Rickshaw', 'Goldfish', 'Kiwi fruit', 'Router/modem', 'Poker Card', 'Toaster', 'Shrimp', 'Sushi', 'Cheese', 'Notepaper', 'Cherry', 'Pliers', 'CD', 'Pasta', 'Hammer', 'Cue', 'Avocado', 'Hamimelon', 'Flask', 'Mushroon', 'Screwdriver', 'Soap', 'Recorder', 'Bear', 'Eggplant', 'Board Eraser', 'Coconut', 'Tape Measur/ Ruler', 'Pig', 'Showerhead', 'Globe', 'Chips', 'Steak', 'Crosswalk Sign', 'Stapler', 'Campel', 'Formula 1 ', 'Pomegranate', 'Dishwasher', 'Crab', 'Hoverboard', 'Meat ball', 'Rice Cooker', 'Tuba', 'Calculator', 'Papaya', 'Antelope', 'Parrot', 'Seal', 'Buttefly', 'Dumbbell', 'Donkey', 'Lion', 'Urinal', 'Dolphin', 'Electric Drill', 'Hair Dryer', 'Egg tart', 'Jellyfish', 'Treadmill', 'Lighter', 'Grapefruit', 'Game board', 'Mop', 'Radish', 'Baozi', 'Target', 'French', 'Spring Rolls', 'Monkey', 'Rabbit', 'Pencil Case', 'Yak', 'Red Cabbage', 'Binoculars', 'Asparagus', 'Barbell', 'Scallop', 'Noddles', 'Comb', 'Dumpling', 'Oyster', 'Table Teniis paddle', 'Cosmetics Brush/Eyeliner Pencil', 'Chainsaw', 'Eraser', 'Lobster', 'Durian', 'Okra', 'Lipstick', 'Cosmetics Mirror', 'Curling', 'Table Tennis ']\n",
      "            üñºÔ∏è Im size: 640\n",
      "            \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from focoos.model_registry import ModelRegistry\n",
    "\n",
    "registry = ModelRegistry()\n",
    "print(registry.list_models())\n",
    "\n",
    "\n",
    "model_info = registry.get_model_info(\"fai-detr-l-obj365\")\n",
    "\n",
    "model_info.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[05/22 07:54][INFO][focoos.data.auto_dataset]: ‚úÖ Dataset name: aquarium, Dataset Path: ../datasets/aquarium, Dataset Layout: DatasetLayout.ROBOFLOW_COCO\u001b[0m\n",
      "\u001b[1;32m[05/22 07:55][INFO][focoos.data.datasets.dict_dataset]: [Focoos-DictDataset] dataset aquarium loaded. len: 448, classes:7 ,../datasets/aquarium/train\u001b[0m\n",
      "\u001b[1;33m[05/22 07:55][DEBUG][focoos.data.datasets.serialize]: Serializing 448 elements to byte tensors and concatenating them all ...\u001b[0m\n",
      "\u001b[1;33m[05/22 07:55][DEBUG][focoos.data.datasets.serialize]: Serialized dataset takes 0.27 MiB\u001b[0m\n",
      "\u001b[1;32m[05/22 07:55][INFO][focoos.data.mappers.detection_dataset_mapper]: [DatasetMapper] Augmentations used in training: [ColorAugSSDTransform(img_format=RGB, brightness_delta=32, contrast_low=0.5, contrast_high=1.5, saturation_low=0.5, saturation_high=1.5, hue_delta=18), RandomFlip(prob=0.5, horizontal=True, vertical=False), RandomApply(prob=1.0, tfm=Resize(shape=(640, 640))), ResizeShortestEdge(short_edge_length=[452, 905], max_size=800, sample_style=range, interp=2), RandomCrop(crop_type=absolute_range, crop_size=(640, 640))]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[05/22 07:55][INFO][focoos.data.datasets.dict_dataset]: [Focoos-DictDataset] dataset aquarium loaded. len: 127, classes:7 ,../datasets/aquarium/valid\u001b[0m\n",
      "\u001b[1;33m[05/22 07:55][DEBUG][focoos.data.datasets.serialize]: Serializing 127 elements to byte tensors and concatenating them all ...\u001b[0m\n",
      "\u001b[1;33m[05/22 07:55][DEBUG][focoos.data.datasets.serialize]: Serialized dataset takes 0.08 MiB\u001b[0m\n",
      "\u001b[1;32m[05/22 07:55][INFO][focoos.data.mappers.detection_dataset_mapper]: [DatasetMapper] Augmentations used in inference: [RandomApply(prob=1.0, tfm=Resize(shape=(640, 640))), ResizeShortestEdge(short_edge_length=[640, 640], max_size=4096, sample_style=range, interp=2)]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.ports import DatasetLayout, DatasetSplitType, Task\n",
    "\n",
    "task = Task.DETECTION\n",
    "layout = DatasetLayout.ROBOFLOW_COCO\n",
    "auto_dataset = AutoDataset(dataset_name=\"aquarium\", task=task, layout=layout, datasets_dir=\"../datasets\")\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(task, 640, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[05/22 07:51][INFO][ModelManager]: Downloading weights from remote URL: https://public.focoos.ai/pretrained_models/fai-detr-m-coco/model_final.pth\u001b[0m\n",
      "\u001b[1;33m[05/22 07:51][DEBUG][HUB]: üì• File already exists: /root/FocoosAI/models/fai-detr-m-coco/model_final.pth\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.criterion.empty_weight' to the model due to incompatible shapes: (81,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.enc_score_classifier.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (7, 256) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.enc_score_classifier.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.dec_score_classifier.0.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (7, 256) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.dec_score_classifier.0.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.dec_score_classifier.1.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (7, 256) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.dec_score_classifier.1.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.dec_score_classifier.2.weight' to the model due to incompatible shapes: (80, 256) in the checkpoint but (7, 256) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Skip loading parameter 'head.predictor.dec_score_classifier.2.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\u001b[0m\n",
      "\u001b[1;35m[05/22 07:51][WARNING][Checkpoint]: Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mhead.criterion.empty_weight\u001b[0m\n",
      "\u001b[34mhead.predictor.dec_score_classifier.0.{bias, weight}\u001b[0m\n",
      "\u001b[34mhead.predictor.dec_score_classifier.1.{bias, weight}\u001b[0m\n",
      "\u001b[34mhead.predictor.dec_score_classifier.2.{bias, weight}\u001b[0m\n",
      "\u001b[34mhead.predictor.enc_score_classifier.{bias, weight}\u001b[0m\u001b[0m\n",
      "\u001b[1;32m[05/22 07:51][INFO][ModelManager]: ‚úÖ Weights loaded for model fai-detr-m-coco\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from focoos.model_manager import ModelManager\n",
    "\n",
    "model = ModelManager.get(\"fai-detr-m-coco\", num_classes=train_dataset.dataset.metadata.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with a dataset downloaded from HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m[05/22 08:03][ERROR][HUB]: API key is required ü§ñ\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "API key is required ü§ñ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfocoos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelManager\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfocoos\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEV_API_URL, DatasetLayout, DatasetSplitType, RuntimeType, Task, TrainerArgs\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m hub = \u001b[43mFocoosHUB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEV_API_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m my_datasets = hub.list_remote_datasets(include_shared=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m remote_dataset = hub.get_remote_dataset(my_datasets[\u001b[32m6\u001b[39m].ref)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/ubuntu/focoos-1/focoos/hub/focoos_hub.py:94\u001b[39m, in \u001b[36mFocoosHUB.__init__\u001b[39m\u001b[34m(self, api_key, host_url)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key:\n\u001b[32m     93\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mAPI key is required ü§ñ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAPI key is required ü§ñ\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m.host_url = host_url \u001b[38;5;129;01mor\u001b[39;00m FOCOOS_CONFIG.default_host_url\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.api_client = ApiClient(api_key=\u001b[38;5;28mself\u001b[39m.api_key, host_url=\u001b[38;5;28mself\u001b[39m.host_url)\n",
      "\u001b[31mValueError\u001b[39m: API key is required ü§ñ"
     ]
    }
   ],
   "source": [
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.hub.focoos_hub import FocoosHUB\n",
    "from focoos.model_manager import ModelManager\n",
    "from focoos.ports import DEV_API_URL, DatasetLayout, DatasetSplitType, RuntimeType, Task, TrainerArgs\n",
    "\n",
    "hub = FocoosHUB(host_url=DEV_API_URL)\n",
    "my_datasets = hub.list_remote_datasets(include_shared=False)\n",
    "remote_dataset = hub.get_remote_dataset(my_datasets[6].ref)\n",
    "dataset_path = remote_dataset.download_data()\n",
    "auto_dataset = AutoDataset(dataset_name=dataset_path, task=remote_dataset.task, layout=remote_dataset.layout)\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(remote_dataset.task, 640, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n",
    "\n",
    "\n",
    "model = ModelManager.get(\"fai-detr-m-coco\", num_classes=train_dataset.dataset.metadata.num_classes)\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=f\"{remote_dataset.name}-{model.model_info.name}\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=500,\n",
    "    eval_period=50,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "\n",
    "model.train(args, train_dataset, valid_dataset)\n",
    "infer = model.export(runtime_type=RuntimeType.TORCHSCRIPT_32)\n",
    "infer.benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.ports import TrainerArgs\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=\"aquarium2\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=300,\n",
    "    eval_period=100,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "model.test(args, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model for inference\n",
    "from PIL import Image\n",
    "\n",
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.model_manager import ModelManager\n",
    "from focoos.ports import DatasetLayout, DatasetSplitType, Task, TrainerArgs\n",
    "\n",
    "task = Task.DETECTION\n",
    "layout = DatasetLayout.ROBOFLOW_COCO\n",
    "auto_dataset = AutoDataset(dataset_name=\"aquarium\", task=task, layout=layout)\n",
    "resolution = 640\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(task, resolution, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n",
    "\n",
    "\n",
    "model = ModelManager.get(\"fai-detr-m-coco\", num_classes=train_dataset.dataset.metadata.num_classes)\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=\"exp1\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=100,\n",
    "    eval_period=100,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "model.train(args, train_dataset, valid_dataset)\n",
    "\n",
    "image = Image.open(\"image.jpg\")\n",
    "\n",
    "outputs = model(image)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model for inference\n",
    "from PIL import Image\n",
    "\n",
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.model_manager import ConfigManager, ModelManager\n",
    "from focoos.nn.backbone.resnet import ResnetConfig\n",
    "from focoos.ports import (\n",
    "    DatasetLayout,\n",
    "    DatasetSplitType,\n",
    "    ModelFamily,\n",
    "    ModelInfo,\n",
    "    Task,\n",
    "    TrainerArgs,\n",
    ")\n",
    "\n",
    "task = Task.CLASSIFICATION\n",
    "layout = DatasetLayout.CLS_FOLDER\n",
    "auto_dataset = AutoDataset(dataset_name=\"hymenoptera\", task=task, layout=layout)\n",
    "resolution = 224\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(task, resolution, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n",
    "\n",
    "\n",
    "# Create a configuration with a ResNet backbone\n",
    "cls_config = ConfigManager.from_dict(\n",
    "    ModelFamily.IMAGE_CLASSIFIER,\n",
    "    {\n",
    "        \"backbone_config\": dict(ResnetConfig(model_type=\"resnet\", depth=50, pretrained=True)),\n",
    "        \"num_classes\": valid_dataset.dataset.metadata.num_classes,\n",
    "        \"resolution\": resolution,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"dropout_rate\": 0.2,\n",
    "    },\n",
    ")\n",
    "\n",
    "model_info = ModelInfo(\n",
    "    name=\"fai-cls-resnet50\",\n",
    "    description=\"ResNet50 model for classification\",\n",
    "    task=Task.CLASSIFICATION,\n",
    "    classes=[\"cat\", \"dog\", \"bird\"],\n",
    "    im_size=224,\n",
    "    model_family=ModelFamily.IMAGE_CLASSIFIER,\n",
    "    config=cls_config,\n",
    ")\n",
    "# Create the model\n",
    "model = ModelManager.get(name=model_info.name, model_info=model_info)\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=\"footballxyz\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=50,\n",
    "    eval_period=100,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "model.train(args, train_dataset, valid_dataset)\n",
    "\n",
    "image = Image.open(\"image.jpg\")\n",
    "outputs = model(image)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[05/21 15:16][INFO][focoos.data.auto_dataset]: ‚úÖ Dataset name: pizza, Dataset Path: /root/FocoosAI/datasets/pizza, Dataset Layout: DatasetLayout.ROBOFLOW_SEG\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset /root/FocoosAI/datasets/pizza not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m auto_dataset = AutoDataset(dataset_name=\u001b[33m\"\u001b[39m\u001b[33mpizza\u001b[39m\u001b[33m\"\u001b[39m, task=task, layout=layout)\n\u001b[32m     12\u001b[39m train_augs, val_augs = get_default_by_task(task, \u001b[32m640\u001b[39m, advanced=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_dataset = \u001b[43mauto_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_augs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_augmentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDatasetSplitType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTRAIN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n\u001b[32m     16\u001b[39m model = ModelManager.get(\u001b[33m\"\u001b[39m\u001b[33mbisenetformer-m-ade\u001b[39m\u001b[33m\"\u001b[39m, num_classes=valid_dataset.dataset.metadata.num_classes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/ubuntu/focoos-1/focoos/data/auto_dataset.py:155\u001b[39m, in \u001b[36mAutoDataset.get_split\u001b[39m\u001b[34m(self, augs, split)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_split\u001b[39m(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    139\u001b[39m     augs: List[T.Transform],\n\u001b[32m    140\u001b[39m     split: DatasetSplitType = DatasetSplitType.TRAIN,\n\u001b[32m    141\u001b[39m ) -> MapDataset:\n\u001b[32m    142\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m    Generate a dataset for a given dataset name with optional augmentations.\u001b[39;00m\n\u001b[32m    144\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m        MapDataset: A DictDataset with DatasetMapper for training.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         dataset=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    156\u001b[39m         mapper=\u001b[38;5;28mself\u001b[39m._load_mapper(\n\u001b[32m    157\u001b[39m             augs=augs,\n\u001b[32m    158\u001b[39m             is_validation_split=(split == DatasetSplitType.VAL),\n\u001b[32m    159\u001b[39m         ),\n\u001b[32m    160\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/ubuntu/focoos-1/focoos/data/auto_dataset.py:72\u001b[39m, in \u001b[36mAutoDataset._load_split\u001b[39m\u001b[34m(self, dataset_name, split)\u001b[39m\n\u001b[32m     70\u001b[39m ds_root = \u001b[38;5;28mself\u001b[39m.dataset_path\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_folder_exists(ds_root):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m split_path = \u001b[38;5;28mself\u001b[39m._get_split_path(dataset_root=ds_root, split_type=split)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layout == DatasetLayout.ROBOFLOW_SEG:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset /root/FocoosAI/datasets/pizza not found"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.model_manager import ModelManager\n",
    "from focoos.ports import DatasetLayout, DatasetSplitType, Task, TrainerArgs\n",
    "\n",
    "task = Task.SEMSEG\n",
    "layout = DatasetLayout.ROBOFLOW_SEG\n",
    "auto_dataset = AutoDataset(dataset_name=\"pizza\", task=task, layout=layout)\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(task, 640, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n",
    "\n",
    "model = ModelManager.get(\"bisenetformer-m-ade\", num_classes=valid_dataset.dataset.metadata.num_classes)\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=\"footballxyz\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=50,\n",
    "    eval_period=100,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "model.train(args, train_dataset, valid_dataset)\n",
    "\n",
    "image = Image.open(\"image.jpg\")\n",
    "outputs = model(image)\n",
    "\n",
    "# print(outputs.logits.shape,outputs.masks.shape)\n",
    "for det in outputs[0].detections:\n",
    "    print(det.cls_id, det.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.model_manager import ModelManager\n",
    "from focoos.ports import DatasetLayout, DatasetSplitType, Task, TrainerArgs\n",
    "\n",
    "task = Task.INSTANCE_SEGMENTATION\n",
    "layout = DatasetLayout.ROBOFLOW_COCO\n",
    "auto_dataset = AutoDataset(dataset_name=\"fruits\", task=task, layout=layout)\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(task, 640, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n",
    "\n",
    "model = ModelManager.get(\"fai-mf-s-coco-ins\", num_classes=valid_dataset.dataset.metadata.num_classes)\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=\"footballxyz\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=50,\n",
    "    eval_period=100,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "model.train(args, train_dataset, valid_dataset)\n",
    "\n",
    "image = Image.open(\"image.jpg\")\n",
    "outputs = model(image)\n",
    "\n",
    "# print(outputs.logits.shape,outputs.masks.shape)\n",
    "for det in outputs[0].detections:\n",
    "    print(det.cls_id, det.bbox, det.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.utils.system import get_system_info\n",
    "\n",
    "get_system_info().pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.model_registry import ModelRegistry\n",
    "\n",
    "registry = ModelRegistry()\n",
    "print(registry.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.model_manager import ModelManager\n",
    "\n",
    "model = ModelManager.get(\"fai-detr-l-obj365\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "from focoos.ports import Task\n",
    "from focoos.utils.vision import fai_detections_to_sv\n",
    "\n",
    "# Initialize annotation utilities\n",
    "label_annotator = sv.LabelAnnotator(text_padding=10, border_radius=10)\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "\n",
    "\n",
    "def annotate(im, model_info, detections):\n",
    "    detections = fai_detections_to_sv(detections, im.shape[:2])\n",
    "    if len(detections.xyxy) == 0:\n",
    "        print(\"No detections found, skipping annotation\")\n",
    "        return im\n",
    "    classes = model_info.classes\n",
    "    labels = [\n",
    "        f\"{classes[int(class_id)] if classes is not None else str(class_id)}: {confid * 100:.0f}%\"\n",
    "        for class_id, confid in zip(detections.class_id, detections.confidence)  # type: ignore\n",
    "    ]\n",
    "    if model_info.task == Task.DETECTION:\n",
    "        annotated_im = box_annotator.annotate(scene=im.copy(), detections=detections)\n",
    "\n",
    "        annotated_im = label_annotator.annotate(scene=annotated_im, detections=detections, labels=labels)\n",
    "    elif model_info.task in [\n",
    "        Task.SEMSEG,\n",
    "        Task.INSTANCE_SEGMENTATION,\n",
    "    ]:\n",
    "        annotated_im = mask_annotator.annotate(scene=im.copy(), detections=detections)\n",
    "\n",
    "    return Image.fromarray(annotated_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table of the benchmarking metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "\n",
    "from focoos.model_manager import ModelManager\n",
    "from focoos.model_registry import ModelRegistry\n",
    "from focoos.ports import RuntimeType\n",
    "\n",
    "registry = ModelRegistry()\n",
    "\n",
    "image = Image.open(\"image.jpg\")\n",
    "\n",
    "model_name = \"fai-detr-s-coco\"\n",
    "# model_name = \"fai-mf-m-coco-ins\"\n",
    "# model_name = \"fai-mf-m-ade\"\n",
    "# model_name = \"bisenetformer-m-ade\"\n",
    "\n",
    "model = ModelManager.get(model_name)\n",
    "\n",
    "metrics_torch = model.benchmark(iterations=50, size=640)\n",
    "metrics_torch_inner = model.model.benchmark(iterations=50, size=640)\n",
    "\n",
    "runtime_type = RuntimeType.TORCHSCRIPT_32\n",
    "infer = model.export(runtime_type=runtime_type, overwrite=True)\n",
    "metrics_trt = infer.benchmark(iterations=50, size=640)\n",
    "metrics_inner_ts = infer.runtime.benchmark(iterations=50, size=640)\n",
    "\n",
    "runtime_type = RuntimeType.ONNX_CUDA32\n",
    "infer = model.export(runtime_type=runtime_type, overwrite=True)\n",
    "metrics_onnx = infer.benchmark(iterations=50, size=640)\n",
    "metrics_inner_onnx = infer.runtime.benchmark(iterations=50, size=640)\n",
    "\n",
    "\n",
    "# Create data for the table\n",
    "headers = [\"Runtime\", \"FPS\", \"Mean Latency (ms)\", \"Std Deviation (ms)\"]\n",
    "table_data = [\n",
    "    [\"PyTorch\", metrics_torch.fps, metrics_torch.mean, metrics_torch.std],\n",
    "    [\"TorchScript\", metrics_trt.fps, metrics_trt.mean, metrics_trt.std],\n",
    "    [\"ONNX CUDA\", metrics_onnx.fps, metrics_onnx.mean, metrics_onnx.std],\n",
    "    [\"PyTorch Model\", metrics_torch_inner.fps, metrics_torch_inner.mean, metrics_torch_inner.std],\n",
    "    [\"TorchScript Model\", metrics_inner_ts.fps, metrics_inner_ts.mean, metrics_inner_ts.std],\n",
    "    [\"ONNX CUDA Model\", metrics_inner_onnx.fps, metrics_inner_onnx.mean, metrics_inner_onnx.std],\n",
    "]\n",
    "\n",
    "# Display the table using tabulate\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "# Optionally, create a bar chart to visualize FPS comparison\n",
    "runtimes = [row[0] for row in table_data]\n",
    "fps_values = [row[1] for row in table_data]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(runtimes, fps_values)\n",
    "plt.title(\"FPS Comparison Across Different Runtimes\")\n",
    "plt.xlabel(\"Runtime\")\n",
    "plt.ylabel(\"Frames Per Second (FPS)\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sync to HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table of the benchmarking metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tabulate import tabulate\n",
    "\n",
    "from focoos.data.auto_dataset import AutoDataset\n",
    "from focoos.data.default_aug import get_default_by_task\n",
    "from focoos.hub import FocoosHUB\n",
    "from focoos.model_manager import ModelManager\n",
    "from focoos.model_registry import ModelRegistry\n",
    "from focoos.ports import DEV_API_URL, LOCAL_API_URL, DatasetLayout, DatasetSplitType, RuntimeType, Task, TrainerArgs\n",
    "\n",
    "hub = FocoosHUB(host_url=LOCAL_API_URL)\n",
    "# my_datasets = hub.list_remote_datasets(include_shared=False)\n",
    "# remote_dataset = hub.get_remote_dataset(my_datasets[6].ref)\n",
    "# dataset_path = remote_dataset.download_data()\n",
    "\n",
    "\n",
    "auto_dataset = AutoDataset(dataset_name=\"Carrera_go_red_grey\", task=Task.DETECTION, layout=DatasetLayout.ROBOFLOW_COCO)\n",
    "\n",
    "train_augs, val_augs = get_default_by_task(Task.DETECTION, 640, advanced=False)\n",
    "train_dataset = auto_dataset.get_split(augs=train_augs.get_augmentations(), split=DatasetSplitType.TRAIN)\n",
    "valid_dataset = auto_dataset.get_split(augs=val_augs.get_augmentations(), split=DatasetSplitType.VAL)\n",
    "\n",
    "model = ModelManager.get(\"fai-detr-l-obj365\")\n",
    "\n",
    "args = TrainerArgs(\n",
    "    run_name=f\"{auto_dataset.name}-{model.model_info.name}-6\",\n",
    "    output_dir=\"./experiments\",\n",
    "    amp_enabled=True,\n",
    "    batch_size=16,\n",
    "    max_iters=1000,\n",
    "    eval_period=100,\n",
    "    learning_rate=0.0001,\n",
    "    scheduler=\"MULTISTEP\",\n",
    "    weight_decay=0.0001,\n",
    "    workers=16,\n",
    "    sync_to_hub=True,\n",
    ")\n",
    "\n",
    "\n",
    "model.train(args, train_dataset, valid_dataset, hub)\n",
    "model.export(runtime_type=RuntimeType.TORCHSCRIPT_32, overwrite=True)\n",
    "# infer = model.export(runtime_type=RuntimeType.TORCHSCRIPT_32, overwrite=True)\n",
    "# infer.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.model_manager import ModelManager\n",
    "from focoos.ports import RuntimeType\n",
    "\n",
    "model = ModelManager.get(\"fai-detr-l-obj365\")\n",
    "model.export(runtime_type=RuntimeType.TORCHSCRIPT_32, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.utils.metrics import parse_metrics\n",
    "\n",
    "metrics = parse_metrics(\"/home/ubuntu/focoos/notebooks/experiments/carrera-fai-detr-m-coco/metrics.json\")\n",
    "print(metrics.iterations)\n",
    "print(metrics.valid_metrics)\n",
    "print(metrics.train_metrics)\n",
    "print(metrics.infer_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.model_manager import ModelManager\n",
    "from focoos.ports import RuntimeType\n",
    "\n",
    "models_dir = \"/home/ubuntu/focoos/notebooks/experiments\"\n",
    "model_name = \"haloaimbot\"\n",
    "\n",
    "model = ModelManager.get(model_name, models_dir=models_dir)\n",
    "infer = model.export(\n",
    "    runtime_type=RuntimeType.TORCHSCRIPT_32,\n",
    "    overwrite=True,\n",
    "    out_dir=f\"{models_dir}/{model_name}\",\n",
    ")\n",
    "infer.benchmark()\n",
    "infer.infer(\"./image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.infer.infer_model import InferModel\n",
    "from focoos.ports import ModelInfo, RuntimeType\n",
    "\n",
    "model_dir = \"/home/ubuntu/FocoosAI/models/fai-detr-l-obj365\"\n",
    "model_dir = \"/home/ubuntu/focoos/notebooks/experiments/Carrera_go_red_grey-fai-detr-l-obj365-6\"\n",
    "model_dir = \"/home/ubuntu/focoos/notebooks/experiments/haloaimbot\"\n",
    "model_info = ModelInfo.from_json(f\"{model_dir}/model_info.json\")\n",
    "print(model_info.config)\n",
    "infer = InferModel(model_dir=model_dir, model_info=model_info, runtime_type=RuntimeType.TORCHSCRIPT_32)\n",
    "infer.infer(\"./image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
