{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêç Setup Focoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'focoos @ git+https://github.com/FocoosAI/focoos.git'\n",
    "\n",
    "# If you want to run it locally using CPU you can install the package with the following command:\n",
    "# %pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'\n",
    "\n",
    "# If you want to run it locally using GPU you can install the package with the following command:\n",
    "# %pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"image.jpg\"):\n",
    "    print(\"Downloading image.jpg\")\n",
    "    !curl https://www.ondacinema.it/images/serial/xl/howimetyourmother-fotoxl.jpg -o image.jpg\n",
    "image_path = \"image.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Playground with Focoos Models\n",
    "\n",
    "See the list of available models on the [Focoos Models](https://focoosai.github.io/focoos/models/) page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Focoos client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from focoos import Focoos, RuntimeTypes\n",
    "\n",
    "focoos = Focoos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote Inference\n",
    "This section demonstrates how to perform remote inference using a model from the Focoos platform.\n",
    "We will load a remote model (can be a pre-trained model or a custom user model), and then run inference on a sample image with focoos API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = \"fai-rtdetr-m-obj365\"\n",
    "\n",
    "model = focoos.get_remote_model(model_ref)\n",
    "\n",
    "output, preview = model.infer(image_path, threshold=0.6, annotate=True)\n",
    "\n",
    "Image.fromarray(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Inference\n",
    "\n",
    "This section demonstrates how to perform local inference using a model from the Focoos platform. \n",
    "We will load a model, deploy it locally, and then run inference on a sample image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß **NOTE**: To run the local inference, you need to install one of the extras modules.\n",
    "# Available Runtimes and Execution Providers\n",
    "\n",
    "| RuntimeType | Extra | Runtime | Compatible Devices | Available ExecutionProvider |\n",
    "|------------|-------|---------|-------------------|---------------------------|\n",
    "| ONNX_CUDA32 | `[cuda]` | onnxruntime CUDA | NVIDIA GPUs | CUDAExecutionProvider |\n",
    "| ONNX_TRT32 | `[tensorrt]` | onnxruntime TRT | NVIDIA GPUs (Optimized) | CUDAExecutionProvider, TensorrtExecutionProvider |\n",
    "| ONNX_TRT16 | `[tensorrt]` | onnxruntime TRT | NVIDIA GPUs (Optimized) | CUDAExecutionProvider, TensorrtExecutionProvider |\n",
    "| ONNX_CPU | `[cpu]` | onnxruntime CPU | CPU (x86, ARM), M1, M2, M3 (Apple Silicon) | CPUExecutionProvider, CoreMLExecutionProvider, AzureExecutionProvider |\n",
    "| ONNX_COREML | `[cpu]` | onnxruntime CPU | M1, M2, M3 (Apple Silicon) | CoreMLExecutionProvider, CPUExecutionProvider |\n",
    "| TORCHSCRIPT_32 | `[torch]` | torchscript | CPU, NVIDIA GPUs | - |\n",
    "\n",
    "To install the extras modules, use the command: \n",
    "\n",
    "```bash \n",
    "pip install 'focoos[{{extra-name}}] @ git+https://github.com/FocoosAI/focoos.git'\n",
    "```\n",
    "\n",
    "# We will use the cpu as an example, feel free to choose the one that best fits your needs\n",
    "%pip install 'focoos[cpu] @ git+https://github.com/FocoosAI/focoos.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the kernel to reload the modules with the new dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with TorchscriptRuntime (CUDA32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the inference, you need to install the torch extra module\n",
    "# %pip install 'focoos[torch] @ git+https://github.com/FocoosAI/focoos.git'\n",
    "# Rerun the kernel to reload the modules with the new dependencies\n",
    "\n",
    "\n",
    "model_ref = \"fai-rtdetr-m-obj365\"\n",
    "\n",
    "model = focoos.get_infer_model(model_ref, runtime_type=RuntimeTypes.TORCHSCRIPT_32)\n",
    "\n",
    "latency = model.benchmark(iterations=10, size=640)\n",
    "pprint(latency)\n",
    "\n",
    "output, preview = model.infer(image_path, threshold=0.6, annotate=True)\n",
    "pprint(output.detections)\n",
    "pprint(output.latency)\n",
    "\n",
    "Image.fromarray(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with OnnxRuntime (CUDA32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the inference, you need to install the torch extra module\n",
    "# %pip install 'focoos[cuda] @ git+https://github.com/FocoosAI/focoos.git'\n",
    "# Rerun the kernel to reload the modules with the new dependencies\n",
    "\n",
    "model_ref = \"fai-rtdetr-m-obj365\"\n",
    "\n",
    "model = focoos.get_infer_model(model_ref, runtime_type=RuntimeTypes.ONNX_CUDA32)\n",
    "\n",
    "latency = model.benchmark(iterations=10, size=640)\n",
    "pprint(latency)\n",
    "# pprint(latency)\n",
    "output, preview = model.infer(image_path, threshold=0.6, annotate=True)\n",
    "pprint(output.detections)\n",
    "pprint(output.latency)\n",
    "\n",
    "Image.fromarray(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with OnnxRuntime (TensorRT) (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the inference, you need to install the torch extra module\n",
    "# %pip install 'focoos[tensorrt] @ git+https://github.com/FocoosAI/focoos.git'\n",
    "# Rerun the kernel to reload the modules with the new dependencies\n",
    "\n",
    "model_ref = \"fai-rtdetr-m-obj365\"\n",
    "\n",
    "model = focoos.get_infer_model(model_ref, runtime_type=RuntimeTypes.ONNX_TRT16)\n",
    "\n",
    "latency = model.benchmark(iterations=10, size=640)\n",
    "pprint(latency)\n",
    "# pprint(latency)\n",
    "output, preview = model.infer(image_path, threshold=0.6, annotate=True)\n",
    "pprint(output.detections)\n",
    "pprint(output.latency)\n",
    "\n",
    "Image.fromarray(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with OnnxRuntime (CoreML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = \"fai-rtdetr-m-obj365\"\n",
    "\n",
    "model = focoos.get_infer_model(model_ref, runtime_type=RuntimeTypes.ONNX_COREML)\n",
    "\n",
    "latency = model.benchmark(iterations=10, size=640)\n",
    "pprint(latency)\n",
    "\n",
    "output, preview = model.infer(image_path, threshold=0.6, annotate=True)\n",
    "pprint(output.detections)\n",
    "pprint(output.latency)\n",
    "\n",
    "Image.fromarray(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with OnnxRuntime (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = \"fai-rtdetr-m-obj365\"\n",
    "\n",
    "model = focoos.get_infer_model(model_ref, runtime_type=RuntimeTypes.ONNX_CPU)\n",
    "\n",
    "latency = model.benchmark(iterations=10, size=640)\n",
    "pprint(latency)\n",
    "\n",
    "output, preview = model.infer(image_path, threshold=0.6, annotate=True)\n",
    "pprint(output.detections)\n",
    "pprint(output.latency)\n",
    "\n",
    "Image.fromarray(preview)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
