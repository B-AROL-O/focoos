{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877d7eb6",
   "metadata": {},
   "source": [
    "# ðŸš€ Quantize classification models for tiny edge deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e2d41",
   "metadata": {},
   "source": [
    "## Get Pretrained Model\n",
    "\n",
    "Focoos offers three pretrained classification models in different sizes:\n",
    "\n",
    "- fai-cls-n-coco  (nano, optimized for Arduino Nicla Vision) \n",
    "- fai-cls-s-coco  (small)\n",
    "- fai-cls-m-coco  (medium)\n",
    "\n",
    "all models are trained on coco dataset at 224px resolution.\n",
    "\n",
    "Choose the model size that best fits your accuracy and efficiency needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6094769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from focoos import ModelManager\n",
    "\n",
    "model_name = \"fai-cls-n-coco\"  # you can also take model from focoos hub with \"hub://YOUR_MODEL_REF\"\n",
    "\n",
    "model = ModelManager.get(model_name)\n",
    "pprint(model.model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a3e20",
   "metadata": {},
   "source": [
    "## Export as optimized ONNX for edge deployment\n",
    "\n",
    "For edge deployment, we need to export model to more portable runtime, like onnxruntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from focoos import ASSETS_DIR, MODELS_DIR, RuntimeType\n",
    "\n",
    "image_size = 96  # 96px input size\n",
    "\n",
    "exported_model = model.export(\n",
    "    runtime_type=RuntimeType.ONNX_CPU,  # optimized for edge or cpu\n",
    "    image_size=image_size,\n",
    "    dynamic_axes=False,  # quantization need static axes!\n",
    "    simplify_onnx=True,  # simplify and optimize onnx model graph\n",
    "    onnx_opset=18,\n",
    "    out_dir=os.path.join(MODELS_DIR, \"my_edge_model\"),\n",
    ")  # save to models dir\n",
    "\n",
    "# benchmark onnx model\n",
    "exported_model.benchmark(iterations=100)\n",
    "\n",
    "# test onnx model\n",
    "im = ASSETS_DIR / \"federer.jpg\"\n",
    "result = exported_model.infer(im, annotate=True)\n",
    "Image.fromarray(result.image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7efca",
   "metadata": {},
   "source": [
    "## Quantize exported model to int8 (or uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos.infer.quantizer import OnnxQuantizer, QuantizationCfg\n",
    "\n",
    "quantization_cfg = QuantizationCfg(\n",
    "    size=image_size,  # input size: must be same as exported model\n",
    "    calibration_images_folder=str(ASSETS_DIR),  # Calibration images folder: It is strongly recommended\n",
    "    # to use the dataset validation split on which the model was trained.\n",
    "    # Here, for example, we will use the assets folder.\n",
    "    format=\"QO\",  # QO (QOperator): All the quantized operators have their own ONNX definitions, like QLinearConv, MatMulInteger etc.\n",
    "    # QDQ (Quantize-DeQuantize): inserts DeQuantizeLinear(QuantizeLinear(tensor)) between the original operators to simulate the quantization and dequantization process.\n",
    "    per_channel=False,  # Per-channel quantization: each channel has its own scale/zero-point â†’ more accurate,\n",
    "    # especially for convolutions, at the cost of extra memory and computation.\n",
    "    normalize_images=True,  # normalize images during preprocessing: some models have normalization outside of model forward\n",
    ")\n",
    "\n",
    "quantizer = OnnxQuantizer(input_model_path=exported_model.model_path, cfg=quantization_cfg)\n",
    "model_path = quantizer.quantize(\n",
    "    benchmark=True  # benchmark bot fp32 and int8 models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249ac23",
   "metadata": {},
   "source": [
    "## Inference with quantized model on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead8c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos import InferModel\n",
    "\n",
    "quantized_model = InferModel(model_path, runtime_type=RuntimeType.ONNX_CPU)\n",
    "\n",
    "res = quantized_model.infer(im, annotate=True)\n",
    "Image.fromarray(res.image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
